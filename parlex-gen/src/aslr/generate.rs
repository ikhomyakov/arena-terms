use super::lexer::{LexContext, Lexer};
use super::parser::{Production, Symbol};
use super::symtab::Symtab;
use super::{parser, slr};
use anyhow::Result;
use chumsky::Parser;
use std::io::Write;
use std::path::Path;

fn capitalize_first(s: &str) -> String {
    let mut c = s.chars();
    match c.next() {
        None => String::new(),
        Some(first) => first.to_uppercase().collect::<String>() + c.as_str(),
    }
}

fn calculate_minimum_unsigned_type(n: usize) -> &'static str {
    assert!(n <= u16::MAX as usize + 1);
    if n <= (u8::MAX as usize) + 1 {
        "u8"
    } else {
        "u16"
    }
}

/// Generate parser code from a grammar file into an output Rust file.
pub fn generate<P: AsRef<Path>>(
    grammar_path: P,
    output_dir: P,
    output_name: String,
    output_debug_info: bool,
) -> Result<()> {
    let output_dir = output_dir.as_ref();
    let grammar = std::fs::read_to_string(&grammar_path)?;
    let mut context = LexContext::default();
    context.prod_labels.add("start");
    context.nonterms.add("Start");
    let toks = Lexer::tokenize_all(&grammar, &mut context);
    let mut prods = parser::parser().parse(&toks).unwrap();
    prods.insert(
        0,
        Production {
            label: Some(0),
            lhs: 0,
            rhs: vec![Symbol::NonTerm(1)],
        },
    );
    context.terms.add("end");

    let mut tokens = Symtab::new();
    tokens.extend(&context.nonterms);
    tokens.extend(&context.terms);

    let n_nonterms = context.nonterms.len();
    let n_terms = context.terms.len();
    let n_tokens = tokens.len();
    let n_prods = prods.len();

    let prodsv: Vec<_> = prods
        .iter()
        .map(|prod| {
            std::iter::once(prod.lhs)
                .chain(prod.rhs.iter().map(|sym| match sym {
                    Symbol::NonTerm(i) => *i,
                    Symbol::Term(i) => *i + n_nonterms,
                }))
                .collect::<Vec<usize>>()
        })
        .collect();

    let mut out = std::fs::File::create(output_dir.join(&output_name).with_extension("rs"))?;

    writeln!(out, "/*")?;
    writeln!(out, "Generated by ASLR.")?;
    writeln!(
        out,
        "Copyright (c) 2005-2025 IKH Software, Inc. <support@ikhsoftware.com>"
    )?;

    let tokens_vec = tokens.vec();

    if output_debug_info {
        writeln!(out, "\n")?;
        slr::write_prods(&mut out, &prodsv, &tokens_vec)?;
        writeln!(out, "\n")?;
    }

    let c = slr::construct_set(&prodsv, n_nonterms, n_terms);
    let n_states = c.len();
    if output_debug_info {
        slr::write_set(&mut out, &c, &prodsv, &tokens_vec)?;
    }

    let (fst, nullable) = slr::first_sets(&prodsv, n_nonterms, n_terms);
    if output_debug_info {
        slr::write_fstflw(&mut out, &fst, Some(&nullable), &tokens_vec)?;
        writeln!(out, "")?;
    }

    let flw = slr::follow_sets(&prodsv, n_nonterms, n_terms, 0, &fst, &nullable);
    if output_debug_info {
        slr::write_fstflw(&mut out, &flw, None, &tokens_vec)?;
        writeln!(out, "")?;
    }
    writeln!(out, "*/\n")?;

    let tab = slr::construct_slr(&c, &flw, &prodsv, n_nonterms, n_terms);

    let n_ambigs: usize = tab
        .iter()
        .flatten()
        .filter(|actions| actions.len() >= 2)
        .count();

    writeln!(
        out,
        "use parlex::{{ParserAction, ParserStateID, ParserAmbigID, ParserProdID, ParserTokenID, ParserData}};\n"
    )?;

    let prod_labels: Vec<_> = prods
        .iter()
        .enumerate()
        .map(|(i, p)| match p.label {
            Some(j) => context.prod_labels.sym(j).unwrap().to_owned(),
            None => format!("rule{}", i),
        })
        .collect();

    let prod_enums: Vec<_> = prod_labels.iter().map(|s| capitalize_first(s)).collect();

    writeln!(out, "#[derive(Debug, Clone, Copy, PartialEq)]")?;
    writeln!(
        out,
        "pub struct StateID({});",
        calculate_minimum_unsigned_type(n_states)
    )?;
    writeln!(out, "impl ParserStateID for StateID {{")?;
    writeln!(out, "    const COUNT: usize = {};", n_states)?;
    writeln!(out, "}}\n")?;
    writeln!(out, "impl From<StateID> for usize {{")?;
    writeln!(out, "    fn from(s: StateID) -> Self {{")?;
    writeln!(out, "        s as usize")?;
    writeln!(out, "    }}")?;
    writeln!(out, "}}\n")?;

    writeln!(out, "#[derive(Debug, Clone, Copy, PartialEq)]")?;
    writeln!(
        out,
        "pub struct AmbigID({});",
        calculate_minimum_unsigned_type(n_ambigs)
    )?;
    writeln!(out, "impl ParserAmbigID for AmbigID {{")?;
    writeln!(out, "    const COUNT: usize = {};", n_ambigs)?;
    writeln!(out, "}}\n")?;
    writeln!(out, "impl From<AmbigID> for usize {{")?;
    writeln!(out, "    fn from(a: AmbigID) -> Self {{")?;
    writeln!(out, "        a as usize")?;
    writeln!(out, "    }}")?;
    writeln!(out, "}}\n")?;

    writeln!(out, "#[derive(Debug, Clone, Copy, PartialEq)]")?;
    writeln!(out, "#[repr({})]", calculate_minimum_unsigned_type(n_prods))?;
    writeln!(out, "pub enum ProdID {{")?;
    for (i, s) in prod_enums.iter().enumerate() {
        writeln!(out, "   {} = {},", s, i)?;
    }
    writeln!(out, "}}\n")?;
    writeln!(out, "impl ParserProdID for ProdID {{")?;
    writeln!(out, "    type TokenID = TokenID;\n")?;
    writeln!(out, "    const COUNT: usize = {};\n", n_prods)?;
    writeln!(out, "    fn label(&self) -> &'static str {{")?;
    writeln!(out, "        ProdID::LABELS[Into::<usize>::into(*self)]")?;
    writeln!(out, "    }}")?;
    writeln!(out, "    fn lhs_token(&self) -> Self::TokenID {{")?;
    writeln!(
        out,
        "        ProdID::LHS_TOKENS[Into::<usize>::into(*self)]"
    )?;
    writeln!(out, "    }}")?;
    writeln!(out, "    fn size(&self) -> usize {{")?;
    writeln!(out, "        ProdID::SIZES[Into::<usize>::into(*self)]")?;
    writeln!(out, "    }}")?;
    writeln!(out, "}}\n")?;
    writeln!(out, "impl From<ProdID> for usize {{")?;
    writeln!(out, "    fn from(p: ProdID) -> Self {{")?;
    writeln!(out, "        p as usize")?;
    writeln!(out, "    }}")?;
    writeln!(out, "}}\n")?;

    writeln!(out, "impl ProdID {{")?;
    writeln!(out, "    pub const LABELS: &'static [&str] = &[")?;
    for (i, s) in prod_labels.iter().enumerate() {
        writeln!(out, "       {:?}, // {}", s, i)?;
    }
    writeln!(out, "    ];\n")?;
    writeln!(out, "    pub const LHS_TOKENS: &[TokenID] = &[")?;
    for (i, p) in prods.iter().enumerate() {
        writeln!(
            out,
            "        TokenID::{}, // {}",
            capitalize_first(context.nonterms.sym(p.lhs).unwrap()),
            i
        )?;
    }
    writeln!(out, "    ];\n")?;
    writeln!(out, "    pub const SIZES: &[usize] = &[")?;
    for (i, p) in prods.iter().enumerate() {
        writeln!(out, "        {}, // {}", p.rhs.len(), i)?;
    }
    writeln!(out, "    ];\n")?;
    writeln!(out, "}}\n")?;

    writeln!(out, "#[derive(Debug, Clone, Copy, PartialEq)]")?;
    writeln!(
        out,
        "#[repr({})]",
        calculate_minimum_unsigned_type(n_tokens + 1)
    )?;
    writeln!(out, "pub enum TokenID {{")?;
    for (i, s) in tokens.iter().enumerate() {
        if i == 0 {
            writeln!(out, "    // Nonterminals:")?;
        }
        if i == n_nonterms {
            writeln!(out, "\n        // Terminals:")?;
        }
        writeln!(out, "    {} = {},", capitalize_first(s), i,)?;
    }
    writeln!(out, "\n        // Error:")?;
    writeln!(out, "    {} = {},", "Error", n_tokens)?;
    writeln!(out, "}}\n")?;
    writeln!(out, "impl ParserTokenID for TokenID {{")?;
    writeln!(out, "    const COUNT_NONTERMINALS: usize = {};", n_nonterms)?;
    writeln!(out, "    const COUNT_TERMINALS: usize = {};", n_terms)?;
    writeln!(
        out,
        "    const COUNT: usize = Self::COUNT_NONTERMINALS + Self::COUNT_TERMINALS + 1;\n"
    )?;
    writeln!(out, "    pub fn label(&self) -> &'static str {{")?;
    writeln!(out, "        TokenID::LABELS[Into::<usize>::into(self)]")?;
    writeln!(out, "    }}")?;
    writeln!(out, "}}\n")?;
    writeln!(out, "impl From<TokenID> for usize {{")?;
    writeln!(out, "    fn from(t: TokenID) -> Self {{")?;
    writeln!(out, "        t as usize")?;
    writeln!(out, "    }}")?;
    writeln!(out, "}}\n")?;

    writeln!(out, "impl TokenID {{")?;
    writeln!(out, "    pub const LABELS: &'static [&str] = &[")?;
    for (i, s) in tokens.iter().enumerate() {
        writeln!(out, "        {:?}, // {}", s, i)?;
    }
    writeln!(out, "        {:?}, // {}", "error", n_tokens)?;
    writeln!(out, "    ];\n")?;
    writeln!(out, "}}\n")?;

    writeln!(
        out,
        "pub type Action = ParserAction<StateID, ProdID, AmbigID>;\n"
    )?;

    writeln!(out, "pub struct Data;")?;
    writeln!(out, "impl Data {{")?;
    writeln!(out, "    const TAB: &'static [[Action; N_TOKENS]] = &[")?;
    let mut ambig = 0;
    let mut s = String::new();
    for (i, row) in tab.iter().enumerate() {
        writeln!(out, "        /* STATE {} */ [", i)?;
        for (j, actions) in row.iter().enumerate() {
            match actions.len() {
                0 => writeln!(
                    out,
                    "            Action::Error, /* {}({}) */",
                    j, tokens_vec[j],
                )?,
                1 => {
                    let act = actions.iter().next().unwrap();
                    writeln!(
                        out,
                        "            Action::{}{}, /* {}({}) */",
                        act.typ.to_str(),
                        match act.val_to_string(&prod_enums) {
                            None => String::new(),
                            Some(s) => format!("({})", s),
                        },
                        j,
                        tokens_vec[j],
                    )?;
                }
                n => {
                    if n > 2 {
                        writeln!(
                            out,
                            "    // ERROR: {} actions (more than 2!) in state {}, token {}({})",
                            n, i, j, tokens_vec[j]
                        )?;
                    }
                    writeln!(
                        out,
                        "            Action::Ambig(AmbigID({})), /* {}({}) */",
                        ambig, j, tokens_vec[j],
                    )?;
                    s.push_str("        [");
                    for act in actions {
                        s.push_str(&format!(
                            "Action::{}({}),",
                            act.typ.to_str(),
                            match act.val_to_string(&prod_enums) {
                                None => String::new(),
                                Some(s) => format!("({})", s),
                            }
                        ));
                    }
                    s.push_str(&format!("], /* {}, {}({}) */\n", i, j, tokens_vec[j]));
                    ambig += 1;
                }
            }
        }
        writeln!(out, "        ],")?;
    }
    writeln!(out, "    ];\n")?;

    writeln!(out, "    const AMBIGS: &'static [[Action; 2]] = &[")?;
    write!(out, "{}", s)?;
    writeln!(out, "    ];\n")?;
    writeln!(out, "}}\n")?;

    writeln!(out, "impl ParserData for Data {{")?;
    writeln!(out, "    type StateID = StateID;")?;
    writeln!(out, "    type AmbigID = AmbigID;")?;
    writeln!(out, "    type TokenID = TokenID;")?;
    writeln!(out, "    type ProdID = ProdID;\n")?;
    writeln!(out, "    fn start_state() -> Self::StateID {{")?;
    writeln!(out, "        StateID(0)")?;
    writeln!(out, "    }}")?;
    writeln!(
        out,
        "    fn lookup(state_id: StateID, token_id: TokenID) -> Action {{"
    )?;
    writeln!(
        out,
        "        Self::TAB[Into::<usize>::into(state_id)][Into::<usize>::into(token_id)]"
    )?;
    writeln!(out, "    }}")?;
    writeln!(
        out,
        "    fn lookup_ambig(ambig_id: AmbigID) -> &'static [Action; 2] {{"
    )?;
    writeln!(out, "        &Self::AMBIGS[Into::<usize>::into(ambig_id)]")?;
    writeln!(out, "    }}")?;
    writeln!(out, "}}\n")?;

    Ok(())
}
